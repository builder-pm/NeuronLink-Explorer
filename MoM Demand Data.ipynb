{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined MySQL and Lakehouse Data Fetching with Dynamic Country Detection\n",
    "\n",
    "This notebook combines data from:\n",
    "1. MySQL database (country-specific schemas)\n",
    "2. AWS Athena Lakehouse (meth_2012 and positions_us_v1 schemas)\n",
    "\n",
    "**Enhanced with dynamic country detection** - automatically detects which countries to fetch from each source:\n",
    "- Fetches from MySQL for countries with individual schemas\n",
    "- Fetches remaining countries from lakehouse meth_2012 + US schema\n",
    "- No manual country lists required!\n",
    "\n",
    "The final output will be a merged dataset with consistent column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import boto3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Date Range Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution date: 20250703\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the start date (YYYY-MM-DD) for the query:  2025-06-01\n",
      "Enter the end date (YYYY-MM-DD) for the query:  2025-06-30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Date range set: 2025-06-01 to 2025-06-30\n"
     ]
    }
   ],
   "source": [
    "# Get execution date for file naming\n",
    "execution_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "print(f\"Execution date: {execution_date}\")\n",
    "\n",
    "def get_date_range_from_user():\n",
    "    \"\"\"\n",
    "    Prompts the user to enter a start and end date and validates their format (YYYY-MM-DD).\n",
    "    Returns a tuple of (start_date, end_date) or (None, None) if input is invalid.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        start_date_str = input(\"Enter the start date (YYYY-MM-DD) for the query: \")\n",
    "        end_date_str = input(\"Enter the end date (YYYY-MM-DD) for the query: \")\n",
    "        try:\n",
    "            start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "            end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "            if start_date > end_date:\n",
    "                print(\"Start date must be before or equal to end date. Please try again.\")\n",
    "                continue\n",
    "            return start_date_str, end_date_str\n",
    "        except ValueError:\n",
    "            print(\"Invalid date format. Please use YYYY-MM-DD.\")\n",
    "\n",
    "def validate_date_range(start_date_str, end_date_str):\n",
    "    \"\"\"\n",
    "    Alternative function for environments where input() is not supported.\n",
    "    Validates the provided start and end date strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"Start date must be before or equal to end date.\")\n",
    "        return start_date_str, end_date_str\n",
    "    except ValueError as e:\n",
    "        print(f\"Date validation error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Try to get dates from user input, fall back to manual entry if needed\n",
    "try:\n",
    "    start_date, end_date = get_date_range_from_user()\n",
    "    print(f\"âœ… Date range set: {start_date} to {end_date}\")\n",
    "except:\n",
    "    print(\"âš ï¸ Interactive input not available. Please manually set your date range below:\")\n",
    "    # Modify these dates as needed\n",
    "    start_date, end_date = validate_date_range('2025-01-01', '2025-12-31')\n",
    "    if start_date and end_date:\n",
    "        print(f\"ðŸ“… Using default date range: {start_date} to {end_date}\")\n",
    "        print(\"ðŸ’¡ To use custom dates, modify the values in the validate_date_range() call above\")\n",
    "    else:\n",
    "        print(\"âŒ Invalid date range. Please check the date format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL Database Connection and Schema Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your MySQL username:  hahhahaha\n",
      "Enter your MySQL password:  hahahahahha\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Interactive input not available for MySQL credentials.\n",
      "ðŸ’¡ Please modify this cell to include your credentials directly if needed.\n"
     ]
    }
   ],
   "source": [
    "def connect_to_mysql():\n",
    "    \"\"\"\n",
    "    Establishes a connection to the MySQL database using provided credentials.\n",
    "    Returns a connection object or None if connection fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        host = 'lookup-unsigned.cluster-ro-cyrwvn3qibpy.us-east-1.rds.amazonaws.com'\n",
    "        user = input(\"Enter your MySQL username: \")\n",
    "        password = input(\"Enter your MySQL password: \")\n",
    "\n",
    "        mydb = mysql.connector.connect(\n",
    "            host=host,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "        print(\"âœ… Successfully connected to MySQL database.\")\n",
    "        return mydb\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"âŒ Error connecting to MySQL: {err}\")\n",
    "        return None\n",
    "    except:\n",
    "        print(\"âš ï¸ Interactive input not available for MySQL credentials.\")\n",
    "        print(\"ðŸ’¡ Please modify this cell to include your credentials directly if needed.\")\n",
    "        return None\n",
    "\n",
    "# Connect to MySQL\n",
    "mysql_conn = connect_to_mysql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found matching schemas (excluding 'positions_us_v1'): positions_au_v1, positions_ca_v1, positions_fr_v1, positions_ph_v1, positions_pt_v1, positions_uk_v1, positions_co_v1, positions_pa_v1, positions_hrf_v1, positions_hk_v1, positions_tw_v1, positions_gh_v1, positions_pe_v1, positions_cl_v1, positions_ec_v1, positions_ma_v1, positions_ng_v1, positions_ke_v1, positions_gt_v1, positions_sv_v1, positions_uy_v1, positions_do_v1, positions_bd_v1, positions_hn_v1, positions_ve_v1, positions_ni_v1, positions_gr_v1, positions_il_v1, positions_qa_v1, positions_bh_v1, positions_hr_v1, positions_ba_v1, positions_kw_v1, positions_bg_v1, positions_mt_v1, positions_pk_v1, positions_sk_v1, positions_ua_v1, positions_eg_v1, positions_om_v1, positions_rs_v1, positions_id_v1, positions_lt_v1, positions_lv_v1, positions_vn_v1\n",
      "ðŸŒ MySQL will fetch data for countries: AU, BA, BD, BG, BH, CA, CL, CO, DO, EC, EG, FR, GB, GH, GR, GT, HK, HN, HR, HRF, ID, IL, KE, KW, LT, LV, MA, MT, NG, NI, OM, PA, PE, PH, PK, PT, QA, RS, SK, SV, TW, UA, UY, VE, VN\n",
      "Found 45 schemas covering 45 countries\n"
     ]
    }
   ],
   "source": [
    "def get_and_filter_schemas(db_connection):\n",
    "    \"\"\"\n",
    "    Queries the database for all distinct schema names and filters them based on\n",
    "    the pattern 'positions_{country_code}_v1', explicitly excluding 'positions_us_v1'.\n",
    "    Returns a list of matching schema names.\n",
    "    \"\"\"\n",
    "    if not db_connection:\n",
    "        return []\n",
    "\n",
    "    matching_schemas = []\n",
    "    cursor = None\n",
    "    try:\n",
    "        cursor = db_connection.cursor()\n",
    "        query = \"SELECT DISTINCT schema_name FROM information_schema.schemata\"\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Regex to match the pattern positions_{country_code}_v1\n",
    "        schema_pattern = re.compile(r\"^positions_([a-z]{2,})_v1$\")\n",
    "\n",
    "        for (schema_name,) in cursor:\n",
    "            if schema_pattern.match(schema_name) and schema_name != \"positions_us_v1\":\n",
    "                matching_schemas.append(schema_name)\n",
    "\n",
    "        if matching_schemas:\n",
    "            print(f\"âœ… Found matching schemas (excluding 'positions_us_v1'): {', '.join(matching_schemas)}\")\n",
    "        else:\n",
    "            print(\"âŒ No schemas found matching the pattern 'positions_{country_code}_v1' (excluding 'positions_us_v1').\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"âŒ Error querying schemas: {err}\")\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "\n",
    "    return matching_schemas\n",
    "\n",
    "def extract_mysql_countries(filtered_schemas):\n",
    "    \"\"\"\n",
    "    Extracts country codes from MySQL schema names.\n",
    "    Returns a list of country codes that will be fetched from MySQL.\n",
    "    \"\"\"\n",
    "    mysql_countries = []\n",
    "    schema_pattern = re.compile(r\"^positions_([a-z]{2,})_v1$\")\n",
    "    \n",
    "    for schema in filtered_schemas:\n",
    "        match = schema_pattern.match(schema)\n",
    "        if match:\n",
    "            country_code = match.group(1).upper()\n",
    "            if country_code == 'UK':\n",
    "                country_code = 'GB'\n",
    "            mysql_countries.append(country_code)\n",
    "    \n",
    "    mysql_countries_sorted = sorted(set(mysql_countries))  # Remove duplicates and sort\n",
    "    print(f\"ðŸŒ MySQL will fetch data for countries: {', '.join(mysql_countries_sorted)}\")\n",
    "    return mysql_countries_sorted\n",
    "\n",
    "# Get filtered schemas and extract countries\n",
    "if mysql_conn:\n",
    "    filtered_schemas = get_and_filter_schemas(mysql_conn)\n",
    "    mysql_countries = extract_mysql_countries(filtered_schemas)\n",
    "    print(f\"Found {len(filtered_schemas)} schemas covering {len(mysql_countries)} countries\")\n",
    "else:\n",
    "    filtered_schemas = []\n",
    "    mysql_countries = []\n",
    "    print(\"âš ï¸ No MySQL connection available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Athena Connection and Dynamic Country Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Setting up AWS credentials...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your aws_access_key_id:  ASIA6EAB4CAJRQKRW2M6\n",
      "Enter your aws_secret_access_key:  iEIreoiYbnnyyBSfRUbPPQsZFbGAMbC4HzfYE7rG\n",
      "Enter your aws_session_token:  IQoJb3JpZ2luX2VjEPP//////////wEaCXVzLWVhc3QtMSJHMEUCIQD6eO59EM0YJa0g3QTkc/Mvhd99we7kKkicYdSNXbtBSgIgOfMzJzVwmIz+uQ9mfqxjSKQ4tSzKk6kbcIPnAueCB+gqogMI7P//////////ARADGgw5NzA2NjY1NDUxNzEiDJlEDpwea6LI3A800Sr2AlPT0LYtqbAh0cm5dj1TvRFv4/DnQXL/Fv51b5EP8mmHKhbkrPhWvTje3dw1j9rMoHIioIcW2HUZRvU62kgbTFO6MHGbaLNbzxqmOyCF424lI1Qzro59bdKVzsOei28qDlYlNyQY1aCRMrd2CABtzr6FPy7NBwY3CbaMxoIJuGcGG0TJ8zKJE/2Btr1Re4ijHodVhHz8yflkGAJPKM8H9bBHbrFqkcscwXDFD9HPeQW+5wbN9lzXt4uIp3EZS9uM6DCTFfxhw8nk++K6f8KGqGmjwbEy7HkkWz4EK18ZiguyKHSlYQjqOM84ES8wIfs9cOYZSZO8jD5/BFC3BAYghYAlc6lG0RVBIHqYdXjhCNMCfFzJJcnW/BG17US4RULPTgn5gY178ijiozUdp6VFnC4y4yVac2581Rc/F8L8tHEf2/XMvf36BV44u1MIn6nxoALkT5015O6D0G670OCMsDsr8hYtlMsDWLgbBGmVF/qTQyc4zXbvMLyWlMMGOqYBy3Vgrk2wwW6jiugfgmrpsm0pnswqce+LczMZJnm/XpTS0TFBHchsKennUsRCFqSu5cq+oEj4oIi1Lt/U7N5lgEQx9a9mcCCm8ZkE4a0XnMtolcAsHjC9xoIBCnrZZuu7b+nWB2kP3CvaPpzgfJaoNpySxieCBrSE2fTsudJTt/URf8icQ/7KQ83EwOX1Nsjgc5NpjcShh8WFn+nSkNUp4dXkarc5Rw==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AWS session and Athena client created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set up AWS credentials\n",
    "print(\"ðŸ” Setting up AWS credentials...\")\n",
    "try:\n",
    "    aws_access_key_id = input(\"Enter your aws_access_key_id: \")\n",
    "    aws_secret_access_key = input(\"Enter your aws_secret_access_key: \")\n",
    "    aws_session_token = input(\"Enter your aws_session_token: \")\n",
    "\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        aws_session_token=aws_session_token,\n",
    "        region_name='us-east-1'\n",
    "    )\n",
    "    \n",
    "    # Create Athena client\n",
    "    athena_client = session.client('athena')\n",
    "    print(\"âœ… AWS session and Athena client created successfully!\")\n",
    "except:\n",
    "    print(\"âš ï¸ Interactive input not available for AWS credentials.\")\n",
    "    print(\"ðŸ’¡ Please modify this cell to include your AWS credentials directly if needed.\")\n",
    "    athena_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mysql_countries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Discover countries to fetch from lakehouse\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m athena_client:\n\u001b[1;32m---> 83\u001b[0m     lakehouse_countries \u001b[38;5;241m=\u001b[39m get_lakehouse_countries(athena_client, \u001b[43mmysql_countries\u001b[49m)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š Data fetching strategy:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   â€¢ MySQL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mysql_countries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m countries via individual schemas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mysql_countries' is not defined"
     ]
    }
   ],
   "source": [
    "def get_lakehouse_countries(athena_client, mysql_countries):\n",
    "    \"\"\"\n",
    "    Queries Athena to get all distinct country codes from meth_2012 schema,\n",
    "    then filters out countries already covered by MySQL schemas.\n",
    "    Returns a list of country codes to fetch from lakehouse.\n",
    "    \"\"\"\n",
    "    if not athena_client:\n",
    "        print(\"âš ï¸ No Athena client available for country discovery\")\n",
    "        return []\n",
    "    \n",
    "    # Query to get distinct countries from meth_2012 schema\n",
    "    discovery_query = \"\"\"\n",
    "    SELECT DISTINCT country_code\n",
    "    FROM \"demand\".\"positions\"\n",
    "    WHERE position_schema = 'meth_2012'\n",
    "      AND country_code IS NOT NULL\n",
    "      AND country_code != ''\n",
    "    ORDER BY country_code\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"ðŸ” Discovering available countries in lakehouse meth_2012 schema...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=discovery_query,\n",
    "            QueryExecutionContext={\n",
    "                'Database': 'demand'\n",
    "            },\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': 's3://naman.kansal/Local Lakehouse Queries/'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        print(f\"Country discovery query ID: {query_execution_id}\")\n",
    "        \n",
    "        # Wait for query to complete\n",
    "        while True:\n",
    "            status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)['QueryExecution']['Status']['State']\n",
    "            if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                break\n",
    "            print(\"â³ Country discovery query running...\")\n",
    "            time.sleep(5)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = round(end_time - start_time, 2)\n",
    "        \n",
    "        if status == 'SUCCEEDED':\n",
    "            print(f\"âœ… Country discovery completed in {execution_time} seconds\")\n",
    "            \n",
    "            # Fetch results\n",
    "            results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "            \n",
    "            # Extract country codes\n",
    "            all_lakehouse_countries = []\n",
    "            for row in results['ResultSet']['Rows'][1:]:  # Skip header\n",
    "                country_code = row['Data'][0].get('VarCharValue', '').strip()\n",
    "                if country_code:\n",
    "                    all_lakehouse_countries.append(country_code)\n",
    "            \n",
    "            print(f\"ðŸŒ Found {len(all_lakehouse_countries)} countries in lakehouse meth_2012 schema\")\n",
    "            print(f\"ðŸ“ All lakehouse countries: {', '.join(sorted(all_lakehouse_countries))}\")\n",
    "            \n",
    "            # Filter out countries already covered by MySQL\n",
    "            countries_to_fetch = [c for c in all_lakehouse_countries if c not in mysql_countries]\n",
    "            countries_to_fetch_sorted = sorted(set(countries_to_fetch))\n",
    "            \n",
    "            print(f\"\\nðŸŽ¯ Lakehouse will fetch data for countries: {', '.join(countries_to_fetch_sorted)}\")\n",
    "            print(f\"ðŸš« Excluded (already in MySQL): {', '.join(sorted(set(mysql_countries)))}\")\n",
    "            \n",
    "            return countries_to_fetch_sorted\n",
    "        else:\n",
    "            print(f\"âŒ Country discovery query failed with status: {status}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during country discovery: {e}\")\n",
    "        return []\n",
    "\n",
    "# Discover countries to fetch from lakehouse\n",
    "if athena_client:\n",
    "    lakehouse_countries = get_lakehouse_countries(athena_client, mysql_countries)\n",
    "    print(f\"\\nðŸ“Š Data fetching strategy:\")\n",
    "    print(f\"   â€¢ MySQL: {len(mysql_countries)} countries via individual schemas\")\n",
    "    print(f\"   â€¢ Lakehouse: {len(lakehouse_countries)} countries via meth_2012 + US via positions_us_v1\")\n",
    "    print(f\"   â€¢ Total unique countries: {len(set(mysql_countries + lakehouse_countries + ['US']))}\")\n",
    "else:\n",
    "    lakehouse_countries = []\n",
    "    print(\"âš ï¸ Skipping lakehouse country discovery - no client available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting MySQL data fetching process...\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_au_v1 (Country: AU)\n",
      "âœ… Successfully fetched 1429 rows from positions_au_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ca_v1 (Country: CA)\n",
      "âœ… Successfully fetched 4070 rows from positions_ca_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_fr_v1 (Country: FR)\n",
      "âœ… Successfully fetched 2231 rows from positions_fr_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ph_v1 (Country: PH)\n",
      "âœ… Successfully fetched 826 rows from positions_ph_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_pt_v1 (Country: PT)\n",
      "âœ… Successfully fetched 586 rows from positions_pt_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_uk_v1 (Country: GB)\n",
      "âœ… Successfully fetched 3161 rows from positions_uk_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_co_v1 (Country: CO)\n",
      "âœ… Successfully fetched 555 rows from positions_co_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_pa_v1 (Country: PA)\n",
      "âœ… Successfully fetched 277 rows from positions_pa_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_hrf_v1 (Country: HRF)\n",
      "âœ… Successfully fetched 1 rows from positions_hrf_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_hk_v1 (Country: HK)\n",
      "âœ… Successfully fetched 494 rows from positions_hk_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_tw_v1 (Country: TW)\n",
      "âœ… Successfully fetched 444 rows from positions_tw_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_gh_v1 (Country: GH)\n",
      "âœ… Successfully fetched 107 rows from positions_gh_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_pe_v1 (Country: PE)\n",
      "âœ… Successfully fetched 373 rows from positions_pe_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_cl_v1 (Country: CL)\n",
      "âœ… Successfully fetched 344 rows from positions_cl_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ec_v1 (Country: EC)\n",
      "âœ… Successfully fetched 220 rows from positions_ec_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ma_v1 (Country: MA)\n",
      "âœ… Successfully fetched 180 rows from positions_ma_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ng_v1 (Country: NG)\n",
      "âœ… Successfully fetched 144 rows from positions_ng_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ke_v1 (Country: KE)\n",
      "âœ… Successfully fetched 307 rows from positions_ke_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_gt_v1 (Country: GT)\n",
      "âœ… Successfully fetched 144 rows from positions_gt_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_sv_v1 (Country: SV)\n",
      "âœ… Successfully fetched 83 rows from positions_sv_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_uy_v1 (Country: UY)\n",
      "âœ… Successfully fetched 120 rows from positions_uy_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_do_v1 (Country: DO)\n",
      "âœ… Successfully fetched 139 rows from positions_do_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_bd_v1 (Country: BD)\n",
      "âœ… Successfully fetched 92 rows from positions_bd_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_hn_v1 (Country: HN)\n",
      "âœ… Successfully fetched 123 rows from positions_hn_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ve_v1 (Country: VE)\n",
      "âœ… Successfully fetched 55 rows from positions_ve_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ni_v1 (Country: NI)\n",
      "âœ… Successfully fetched 181 rows from positions_ni_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_gr_v1 (Country: GR)\n",
      "âœ… Successfully fetched 329 rows from positions_gr_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_il_v1 (Country: IL)\n",
      "âœ… Successfully fetched 225 rows from positions_il_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_qa_v1 (Country: QA)\n",
      "âœ… Successfully fetched 163 rows from positions_qa_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_bh_v1 (Country: BH)\n",
      "âœ… Successfully fetched 81 rows from positions_bh_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_hr_v1 (Country: HR)\n",
      "âœ… Successfully fetched 163 rows from positions_hr_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ba_v1 (Country: BA)\n",
      "âœ… Successfully fetched 43 rows from positions_ba_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_kw_v1 (Country: KW)\n",
      "âœ… Successfully fetched 112 rows from positions_kw_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_bg_v1 (Country: BG)\n",
      "âœ… Successfully fetched 264 rows from positions_bg_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_mt_v1 (Country: MT)\n",
      "âœ… Successfully fetched 98 rows from positions_mt_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_pk_v1 (Country: PK)\n",
      "âœ… Successfully fetched 165 rows from positions_pk_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_sk_v1 (Country: SK)\n",
      "âœ… Successfully fetched 247 rows from positions_sk_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_ua_v1 (Country: UA)\n",
      "âœ… Successfully fetched 227 rows from positions_ua_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_eg_v1 (Country: EG)\n",
      "âœ… Successfully fetched 321 rows from positions_eg_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_om_v1 (Country: OM)\n",
      "âœ… Successfully fetched 123 rows from positions_om_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_rs_v1 (Country: RS)\n",
      "âœ… Successfully fetched 198 rows from positions_rs_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_id_v1 (Country: ID)\n",
      "âœ… Successfully fetched 1235 rows from positions_id_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_lt_v1 (Country: LT)\n",
      "âœ… Successfully fetched 189 rows from positions_lt_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_lv_v1 (Country: LV)\n",
      "âœ… Successfully fetched 111 rows from positions_lv_v1.\n",
      "\n",
      "ðŸ”„ Executing query for schema: positions_vn_v1 (Country: VN)\n",
      "âœ… Successfully fetched 413 rows from positions_vn_v1.\n",
      "\n",
      "ðŸ“Š MySQL Results: 21393 total rows\n",
      "MySQL Data Preview:\n",
      "  country_code position_month  source_id language  total_jobs  \\\n",
      "0           AU         Jun-25        461       en           2   \n",
      "1           AU         Jun-25        757       en           3   \n",
      "2           AU         Jun-25        906       en           1   \n",
      "3           AU         Jun-25        990       en           3   \n",
      "4           AU         Jun-25        991       en           1   \n",
      "\n",
      "   total_positions data_source  \n",
      "0                2       mysql  \n",
      "1                3       mysql  \n",
      "2                1       mysql  \n",
      "3                2       mysql  \n",
      "4                1       mysql  \n"
     ]
    }
   ],
   "source": [
    "def fetch_mysql_data(db_connection, schema_list, start_date_str, end_date_str):\n",
    "    \"\"\"\n",
    "    For each schema in the list, constructs and executes a SQL query\n",
    "    with date range filtering to fetch job and position data.\n",
    "    Returns a pandas DataFrame containing all results and a timing report DataFrame.\n",
    "    \"\"\"\n",
    "    if not db_connection or not schema_list:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    all_data_frames = []\n",
    "    timing_records = []\n",
    "    schema_pattern_for_country_code = re.compile(r\"^positions_([a-z]{2,})_v1$\")\n",
    "\n",
    "    for schema_name in schema_list:\n",
    "        match = schema_pattern_for_country_code.match(schema_name)\n",
    "        if not match:\n",
    "            print(\n",
    "                f\"âš ï¸ Skipping schema '{schema_name}' as it doesn't match expected pattern \"\n",
    "                \"for country code extraction.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        country_code_lower = match.group(1)\n",
    "        if country_code_lower == \"uk\":\n",
    "            country_code_upper = \"GB\"\n",
    "        else:\n",
    "            country_code_upper = country_code_lower.upper()\n",
    "\n",
    "        # Timing start\n",
    "        start_time = datetime.now()\n",
    "        start_time_unix = time.time()\n",
    "\n",
    "        # Updated SQL query with date range filtering\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT\n",
    "            '{country_code_upper}' AS country_code,\n",
    "            DATE_FORMAT(b.date, '%b-%y') AS position_month,\n",
    "            a.covering_source_id as source_id,\n",
    "            jl.`language` as `language`, \n",
    "            COUNT(DISTINCT a.job_id) AS total_jobs,\n",
    "            COUNT(DISTINCT a.position_id) AS total_positions,\n",
    "            'mysql' as data_source\n",
    "        FROM\n",
    "            {schema_name}.long_term_jobs_m2012_new a\n",
    "            LEFT JOIN norm_prod.dates b ON (a.ad_first_seen_date_id = b.id)\n",
    "            LEFT JOIN jobs.tbl_JobLanguage jl ON (a.job_id = jl.jobid)\n",
    "            LEFT JOIN acqnotes.sources c ON (a.covering_source_id = c.id)\n",
    "        WHERE 1=1\n",
    "            AND b.year = 2025\n",
    "            AND b.date >= '{start_date_str}'\n",
    "            AND b.date <= '{end_date_str}'\n",
    "        GROUP BY\n",
    "            1, 2, 3, 4;\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\nðŸ”„ Executing query for schema: {schema_name} (Country: {country_code_upper})\")\n",
    "        try:\n",
    "            df_schema = pd.read_sql_query(sql_query, db_connection)\n",
    "            end_time = datetime.now()\n",
    "            end_time_unix = time.time()\n",
    "            total_processing_time = end_time_unix - start_time_unix\n",
    "\n",
    "            timing_records.append({\n",
    "                \"schema\": schema_name,\n",
    "                \"country_code\": country_code_upper,\n",
    "                \"start_time\": start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"end_time\": end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"total_processing_seconds\": round(total_processing_time, 2),\n",
    "                \"data_source\": \"mysql\",\n",
    "                \"date_range\": f\"{start_date_str} to {end_date_str}\"\n",
    "            })\n",
    "\n",
    "            if not df_schema.empty:\n",
    "                print(f\"âœ… Successfully fetched {len(df_schema)} rows from {schema_name}.\")\n",
    "                all_data_frames.append(df_schema)\n",
    "            else:\n",
    "                print(f\"âš ï¸ No data returned from {schema_name} for the given date range.\")\n",
    "        except Exception as e:\n",
    "            end_time = datetime.now()\n",
    "            end_time_unix = time.time()\n",
    "            total_processing_time = end_time_unix - start_time_unix\n",
    "\n",
    "            timing_records.append({\n",
    "                \"schema\": schema_name,\n",
    "                \"country_code\": country_code_upper,\n",
    "                \"start_time\": start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"end_time\": end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"total_processing_seconds\": round(total_processing_time, 2),\n",
    "                \"error\": str(e),\n",
    "                \"data_source\": \"mysql\",\n",
    "                \"date_range\": f\"{start_date_str} to {end_date_str}\"\n",
    "            })\n",
    "\n",
    "            print(f\"âŒ Error executing query for schema {schema_name}: {e}\")\n",
    "\n",
    "    if not all_data_frames:\n",
    "        print(\"âš ï¸ No data collected from any MySQL schemas.\")\n",
    "        return pd.DataFrame(), pd.DataFrame(timing_records)\n",
    "\n",
    "    final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "    timing_df = pd.DataFrame(timing_records)\n",
    "    return final_df, timing_df\n",
    "\n",
    "# Fetch MySQL data\n",
    "if mysql_conn and filtered_schemas and start_date and end_date:\n",
    "    print(\"\\nðŸš€ Starting MySQL data fetching process...\")\n",
    "    mysql_results_df, mysql_timing_df = fetch_mysql_data(mysql_conn, filtered_schemas, start_date, end_date)\n",
    "    print(f\"\\nðŸ“Š MySQL Results: {len(mysql_results_df)} total rows\")\n",
    "    if not mysql_results_df.empty:\n",
    "        print(\"MySQL Data Preview:\")\n",
    "        print(mysql_results_df.head())\n",
    "else:\n",
    "    mysql_results_df = pd.DataFrame()\n",
    "    mysql_timing_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ Skipping MySQL data fetch - no connection, schemas, or date range available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lakehouse Data Fetching with Dynamic Country List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Athena query will fetch:\n",
      "   â€¢ meth_2012 countries: AD, AE, AF, AG, AI, AL, AM, AO, AQ, AR, AS, AT, AW, AX, AZ, BB, BE, BF, BI, BJ, BL, BM, BN, BO, BQ, BR, BS, BT, BV, BW, BY, BZ, CC, CD, CF, CG, CH, CI, CK, CM, CN, CP, CR, CU, CV, CW, CX, CY, CZ, DE, DJ, DK, DM, DZ, EE, EH, ER, ES, ET, FI, FJ, FK, FM, FO, GA, GD, GE, GF, GG, GI, GL, GM, GN, GP, GQ, GS, GU, GW, GY, HM, HT, HU, IE, IM, IN, IO, IQ, IR, IS, IT, JE, JM, JO, JP, KG, KH, KI, KM, KN, KP, KR, KY, KZ, LA, LB, LC, LI, LK, LR, LS, LU, LY, MC, MD, ME, MF, MG, MH, MK, ML, MM, MN, MP, MQ, MR, MS, MU, MV, MW, MX, MY, MZ, NA, NC, NE, NF, NL, NO, NP, NR, NU, NZ, PF, PG, PL, PM, PN, PS, PW, PY, RE, RO, RU, RW, SA, SB, SC, SD, SE, SG, SH, SI, SJ, SL, SM, SN, SO, SR, SS, ST, SX, SY, SZ, TC, TD, TF, TG, TH, TJ, TK, TL, TM, TN, TO, TR, TT, TV, TZ, UG, UM, US, UZ, VA, VC, VG, VI, VU, WF, WS, YE, YT, ZA, ZM, ZW\n",
      "   â€¢ positions_us_v1: US data\n",
      "   â€¢ Date range: 2025-06-01 to 2025-06-30\n",
      "\n",
      "ðŸš€ Starting Athena query execution...\n",
      "Query execution ID: 56103e56-087d-4a9e-8f2d-88314f69dabb\n",
      "â³ Query is still running...\n",
      "â³ Query is still running...\n",
      "â³ Query is still running...\n",
      "âœ… Query completed successfully in 47.07 seconds\n",
      "ðŸ“Š Athena Results: 999 total rows\n",
      "ðŸŒ Countries successfully fetched: AE, AR\n",
      "\n",
      "Athena Data Preview:\n",
      "  country_code position_month source_id        source_name source_country  \\\n",
      "0           AE         Jun-25    100153  FedEx Corporation                  \n",
      "1           AE         Jun-25    152154             GovCIO             US   \n",
      "2           AE         Jun-25    141648               Suez                  \n",
      "3           AE         Jun-25    140741            Tanqeeb                  \n",
      "4           AE         Jun-25    152691              Knauf                  \n",
      "\n",
      "  language total_jobs total_positions data_source  \n",
      "0       en          4               4      athena  \n",
      "1       en          2               2      athena  \n",
      "2       en          4               4      athena  \n",
      "3       ar          4               4      athena  \n",
      "4       en          1               1      athena  \n"
     ]
    }
   ],
   "source": [
    "def build_dynamic_athena_query(start_date_str, end_date_str, lakehouse_countries):\n",
    "    \"\"\"\n",
    "    Builds the Athena query with dynamically determined country list.\n",
    "    \"\"\"\n",
    "    # Convert lakehouse_countries list to SQL format\n",
    "    if lakehouse_countries:\n",
    "        country_list_sql = \"', '\".join(lakehouse_countries)\n",
    "        country_filter = f\"'{ country_list_sql }'\"\n",
    "    else:\n",
    "        # If no countries to fetch from meth_2012, use empty condition that will be false\n",
    "        country_filter = \"'__NO_COUNTRIES__'\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH filtered_positions AS (\n",
    "      SELECT\n",
    "        p.position_id,\n",
    "        p.preferred_job_id,\n",
    "        p.position_schema,\n",
    "        p.country_code,\n",
    "        p.job_id_list\n",
    "      FROM \"demand\".\"positions\" p\n",
    "      WHERE p.year = 2025\n",
    "        AND p.position_schema IN ('meth_2012', 'positions_us_v1')\n",
    "    ),\n",
    "    exploded_jobs AS (\n",
    "      SELECT\n",
    "        fp.position_id,\n",
    "        fp.position_schema,\n",
    "        fp.country_code,\n",
    "        fp.preferred_job_id,\n",
    "        job_id\n",
    "      FROM filtered_positions AS fp\n",
    "      CROSS JOIN UNNEST(fp.job_id_list) AS t(job_id)\n",
    "    )\n",
    "    SELECT\n",
    "      e.country_code,\n",
    "      date_format(\n",
    "        date_trunc('month', j.first_seen_date),\n",
    "        '%b-%y'\n",
    "      ) AS position_month,\n",
    "      j.source_id,\n",
    "      j.source_name,\n",
    "      j.source_country,\n",
    "      jl.language,\n",
    "      COUNT(DISTINCT e.job_id) AS total_jobs,\n",
    "      COUNT(DISTINCT e.position_id) AS total_positions,\n",
    "      'athena' as data_source\n",
    "    FROM exploded_jobs AS e\n",
    "    JOIN \"demand\".\"jobs\" j\n",
    "      ON e.job_id = j.job_id\n",
    "    JOIN \"demand\".\"job_languages\" jl ON (jl.job_id = j.job_id)\n",
    "    WHERE\n",
    "      j.first_seen_date >= date '{start_date_str}'\n",
    "      AND j.first_seen_date <= date '{end_date_str}'\n",
    "      AND (\n",
    "        (\n",
    "          e.position_schema = 'meth_2012'\n",
    "            AND e.country_code IN ({country_filter})\n",
    "        )\n",
    "        OR\n",
    "        e.position_schema = 'positions_us_v1'\n",
    "      )\n",
    "    GROUP BY\n",
    "      e.country_code,\n",
    "      date_trunc('month', j.first_seen_date),\n",
    "      j.source_id,\n",
    "      j.source_name,\n",
    "      j.source_country,\n",
    "      jl.language\n",
    "    ORDER BY\n",
    "      e.country_code,\n",
    "      date_trunc('month', j.first_seen_date);\n",
    "    \"\"\"\n",
    "    \n",
    "    return query\n",
    "\n",
    "def fetch_athena_data(athena_client, start_date_str, end_date_str, lakehouse_countries):\n",
    "    \"\"\"\n",
    "    Executes Athena query with dynamically determined country list and returns results as DataFrame\n",
    "    \"\"\"\n",
    "    if not athena_client:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Build dynamic query\n",
    "    query = build_dynamic_athena_query(start_date_str, end_date_str, lakehouse_countries)\n",
    "    \n",
    "    # Print query info\n",
    "    print(f\"\\nðŸŽ¯ Athena query will fetch:\")\n",
    "    print(f\"   â€¢ meth_2012 countries: {', '.join(lakehouse_countries) if lakehouse_countries else 'None'}\")\n",
    "    print(f\"   â€¢ positions_us_v1: US data\")\n",
    "    print(f\"   â€¢ Date range: {start_date_str} to {end_date_str}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nðŸš€ Starting Athena query execution...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            QueryExecutionContext={\n",
    "                'Database': 'demand'\n",
    "            },\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': 's3://naman.kansal/Local Lakehouse Queries/'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Get query execution ID\n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        print(f\"Query execution ID: {query_execution_id}\")\n",
    "        \n",
    "        # Wait for query to complete\n",
    "        while True:\n",
    "            status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)['QueryExecution']['Status']['State']\n",
    "            if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                break\n",
    "            print(\"â³ Query is still running...\")\n",
    "            time.sleep(15)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = round(end_time - start_time, 2)\n",
    "        \n",
    "        # Check if the query succeeded\n",
    "        if status == 'SUCCEEDED':\n",
    "            print(f\"âœ… Query completed successfully in {execution_time} seconds\")\n",
    "            \n",
    "            # Fetch results\n",
    "            results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "            \n",
    "            # Extract data from results\n",
    "            columns = [col['Label'] for col in results['ResultSet']['ResultSetMetadata']['ColumnInfo']]\n",
    "            rows = [row['Data'] for row in results['ResultSet']['Rows'][1:]]  # Skip header row\n",
    "            data = [[col['VarCharValue'] if 'VarCharValue' in col else None for col in row] for row in rows]\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            print(f\"ðŸ“Š Athena Results: {len(df)} total rows\")\n",
    "            \n",
    "            if not df.empty:\n",
    "                countries_fetched = sorted(df['country_code'].unique())\n",
    "                print(f\"ðŸŒ Countries successfully fetched: {', '.join(countries_fetched)}\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(f\"âŒ Query failed with status: {status}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error executing Athena query: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch Athena data with dynamic country list\n",
    "if athena_client and start_date and end_date:\n",
    "    athena_results_df = fetch_athena_data(athena_client, start_date, end_date, lakehouse_countries)\n",
    "    if not athena_results_df.empty:\n",
    "        print(\"\\nAthena Data Preview:\")\n",
    "        print(athena_results_df.head())\n",
    "else:\n",
    "    athena_results_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ Skipping Athena data fetch - no client or date range available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Harmonization and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Harmonizing and merging datasets...\n",
      "âœ… MySQL data harmonized: 21393 rows\n",
      "âœ… Athena data harmonized: 999 rows\n",
      "\n",
      "ðŸŽ‰ Successfully combined datasets!\n",
      "ðŸ“Š Total combined rows: 22392\n",
      "ðŸ“Š MySQL rows: 21393\n",
      "ðŸ“Š Athena rows: 999\n",
      "ðŸ“… Date range analyzed: 2025-06-01 to 2025-06-30\n",
      "\n",
      "ðŸ” Data completeness check:\n",
      "   Expected countries: AD, AE, AF, AG, AI, AL, AM, AO, AQ, AR, AS, AT, AU, AW, AX, AZ, BA, BB, BD, BE, BF, BG, BH, BI, BJ, BL, BM, BN, BO, BQ, BR, BS, BT, BV, BW, BY, BZ, CA, CC, CD, CF, CG, CH, CI, CK, CL, CM, CN, CO, CP, CR, CU, CV, CW, CX, CY, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, EH, ER, ES, ET, FI, FJ, FK, FM, FO, FR, GA, GB, GD, GE, GF, GG, GH, GI, GL, GM, GN, GP, GQ, GR, GS, GT, GU, GW, GY, HK, HM, HN, HR, HRF, HT, HU, ID, IE, IL, IM, IN, IO, IQ, IR, IS, IT, JE, JM, JO, JP, KE, KG, KH, KI, KM, KN, KP, KR, KW, KY, KZ, LA, LB, LC, LI, LK, LR, LS, LT, LU, LV, LY, MA, MC, MD, ME, MF, MG, MH, MK, ML, MM, MN, MP, MQ, MR, MS, MT, MU, MV, MW, MX, MY, MZ, NA, NC, NE, NF, NG, NI, NL, NO, NP, NR, NU, NZ, OM, PA, PE, PF, PG, PH, PK, PL, PM, PN, PS, PT, PW, PY, QA, RE, RO, RS, RU, RW, SA, SB, SC, SD, SE, SG, SH, SI, SJ, SK, SL, SM, SN, SO, SR, SS, ST, SV, SX, SY, SZ, TC, TD, TF, TG, TH, TJ, TK, TL, TM, TN, TO, TR, TT, TV, TW, TZ, UA, UG, UM, US, UY, UZ, VA, VC, VE, VG, VI, VN, VU, WF, WS, YE, YT, ZA, ZM, ZW\n",
      "   Actual countries in data: AE, AR, AU, BA, BD, BG, BH, CA, CL, CO, DO, EC, EG, FR, GB, GH, GR, GT, HK, HN, HR, HRF, ID, IL, KE, KW, LT, LV, MA, MT, NG, NI, OM, PA, PE, PH, PK, PT, QA, RS, SK, SV, TW, UA, UY, VE, VN\n",
      "   âš ï¸ Missing countries: AD, AF, AG, AI, AL, AM, AO, AQ, AS, AT, AW, AX, AZ, BB, BE, BF, BI, BJ, BL, BM, BN, BO, BQ, BR, BS, BT, BV, BW, BY, BZ, CC, CD, CF, CG, CH, CI, CK, CM, CN, CP, CR, CU, CV, CW, CX, CY, CZ, DE, DJ, DK, DM, DZ, EE, EH, ER, ES, ET, FI, FJ, FK, FM, FO, GA, GD, GE, GF, GG, GI, GL, GM, GN, GP, GQ, GS, GU, GW, GY, HM, HT, HU, IE, IM, IN, IO, IQ, IR, IS, IT, JE, JM, JO, JP, KG, KH, KI, KM, KN, KP, KR, KY, KZ, LA, LB, LC, LI, LK, LR, LS, LU, LY, MC, MD, ME, MF, MG, MH, MK, ML, MM, MN, MP, MQ, MR, MS, MU, MV, MW, MX, MY, MZ, NA, NC, NE, NF, NL, NO, NP, NR, NU, NZ, PF, PG, PL, PM, PN, PS, PW, PY, RE, RO, RU, RW, SA, SB, SC, SD, SE, SG, SH, SI, SJ, SL, SM, SN, SO, SR, SS, ST, SX, SY, SZ, TC, TD, TF, TG, TH, TJ, TK, TL, TM, TN, TO, TR, TT, TV, TZ, UG, UM, US, UZ, VA, VC, VG, VI, VU, WF, WS, YE, YT, ZA, ZM, ZW\n",
      "\n",
      "ðŸ“‹ Combined Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22392 entries, 0 to 22391\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   country_code     22392 non-null  object\n",
      " 1   position_month   22392 non-null  object\n",
      " 2   source_id        22392 non-null  int64 \n",
      " 3   source_name      999 non-null    object\n",
      " 4   source_country   999 non-null    object\n",
      " 5   language         22392 non-null  object\n",
      " 6   total_jobs       22392 non-null  int64 \n",
      " 7   total_positions  22392 non-null  int64 \n",
      " 8   data_source      22392 non-null  object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "\n",
      "ðŸ“Š Combined Data Preview:\n",
      "  country_code position_month  source_id source_name source_country language  \\\n",
      "0           AU         Jun-25        461        None           None       en   \n",
      "1           AU         Jun-25        757        None           None       en   \n",
      "2           AU         Jun-25        906        None           None       en   \n",
      "3           AU         Jun-25        990        None           None       en   \n",
      "4           AU         Jun-25        991        None           None       en   \n",
      "5           AU         Jun-25       1093        None           None       en   \n",
      "6           AU         Jun-25       1098        None           None       en   \n",
      "7           AU         Jun-25       1110        None           None       en   \n",
      "8           AU         Jun-25       1334        None           None       en   \n",
      "9           AU         Jun-25       1348        None           None       en   \n",
      "\n",
      "   total_jobs  total_positions data_source  \n",
      "0           2                2       mysql  \n",
      "1           3                3       mysql  \n",
      "2           1                1       mysql  \n",
      "3           3                2       mysql  \n",
      "4           1                1       mysql  \n",
      "5          43               39       mysql  \n",
      "6          19               19       mysql  \n",
      "7          41               41       mysql  \n",
      "8          16               16       mysql  \n",
      "9          29               29       mysql  \n",
      "\n",
      "ðŸ“ˆ Data Source Distribution:\n",
      "mysql     21393\n",
      "athena      999\n",
      "Name: data_source, dtype: int64\n",
      "\n",
      "ðŸ“ˆ Date Range Coverage:\n",
      "Jun-25    22392\n",
      "Name: position_month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def harmonize_and_merge_data(mysql_df, athena_df):\n",
    "    \"\"\"\n",
    "    Harmonizes column names and data types, then merges the two DataFrames\n",
    "    \"\"\"\n",
    "    combined_dfs = []\n",
    "    \n",
    "    # Process MySQL data\n",
    "    if not mysql_df.empty:\n",
    "        mysql_harmonized = mysql_df.copy()\n",
    "        # Add missing columns with None values\n",
    "        mysql_harmonized['source_name'] = None\n",
    "        mysql_harmonized['source_country'] = None\n",
    "        print(f\"âœ… MySQL data harmonized: {len(mysql_harmonized)} rows\")\n",
    "        combined_dfs.append(mysql_harmonized)\n",
    "    \n",
    "    # Process Athena data  \n",
    "    if not athena_df.empty:\n",
    "        athena_harmonized = athena_df.copy()\n",
    "        print(f\"âœ… Athena data harmonized: {len(athena_harmonized)} rows\")\n",
    "        combined_dfs.append(athena_harmonized)\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    if combined_dfs:\n",
    "        final_combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "        \n",
    "        # Ensure consistent column order\n",
    "        column_order = [\n",
    "            'country_code', 'position_month', 'source_id', 'source_name', \n",
    "            'source_country', 'language', 'total_jobs', 'total_positions', 'data_source'\n",
    "        ]\n",
    "        \n",
    "        final_combined_df = final_combined_df.reindex(columns=column_order)\n",
    "        \n",
    "        # Convert numeric columns\n",
    "        numeric_columns = ['source_id', 'total_jobs', 'total_positions']\n",
    "        for col in numeric_columns:\n",
    "            if col in final_combined_df.columns:\n",
    "                final_combined_df[col] = pd.to_numeric(final_combined_df[col], errors='coerce')\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Successfully combined datasets!\")\n",
    "        print(f\"ðŸ“Š Total combined rows: {len(final_combined_df)}\")\n",
    "        print(f\"ðŸ“Š MySQL rows: {len(mysql_df) if not mysql_df.empty else 0}\")\n",
    "        print(f\"ðŸ“Š Athena rows: {len(athena_df) if not athena_df.empty else 0}\")\n",
    "        print(f\"ðŸ“… Date range analyzed: {start_date} to {end_date}\")\n",
    "        \n",
    "        # Check for data completeness\n",
    "        all_countries_in_data = sorted(final_combined_df['country_code'].unique())\n",
    "        expected_countries = sorted(set(mysql_countries + lakehouse_countries + ['US']))\n",
    "        \n",
    "        print(f\"\\nðŸ” Data completeness check:\")\n",
    "        print(f\"   Expected countries: {', '.join(expected_countries)}\")\n",
    "        print(f\"   Actual countries in data: {', '.join(all_countries_in_data)}\")\n",
    "        \n",
    "        missing_countries = set(expected_countries) - set(all_countries_in_data)\n",
    "        if missing_countries:\n",
    "            print(f\"   âš ï¸ Missing countries: {', '.join(sorted(missing_countries))}\")\n",
    "        else:\n",
    "            print(f\"   âœ… All expected countries present in data\")\n",
    "        \n",
    "        return final_combined_df\n",
    "    else:\n",
    "        print(\"âš ï¸ No data available to combine\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Harmonize and merge data\n",
    "print(\"\\nðŸ”„ Harmonizing and merging datasets...\")\n",
    "combined_final_df = harmonize_and_merge_data(mysql_results_df, athena_results_df)\n",
    "\n",
    "if not combined_final_df.empty:\n",
    "    print(\"\\nðŸ“‹ Combined Dataset Info:\")\n",
    "    print(combined_final_df.info())\n",
    "    print(\"\\nðŸ“Š Combined Data Preview:\")\n",
    "    print(combined_final_df.head(10))\n",
    "    print(\"\\nðŸ“ˆ Data Source Distribution:\")\n",
    "    print(combined_final_df['data_source'].value_counts())\n",
    "    print(\"\\nðŸ“ˆ Date Range Coverage:\")\n",
    "    print(combined_final_df['position_month'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined dataset saved to: Combined_MoM_Country_Source_Volume_Dynamic_20250601_20250630.csv\n",
      "âœ… Country mapping saved to: Country_Source_Mapping_20250601_20250630.csv\n",
      "âœ… Summary statistics saved to: Summary_Stats_Dynamic_20250601_20250630.csv\n",
      "âœ… Monthly trends saved to: Monthly_Trends_Dynamic_20250601_20250630.csv\n",
      "âœ… MySQL timing report saved to: MySQL_Timing_Report_Dynamic_20250601_20250630.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate date range suffix for filenames\n",
    "date_range_suffix = f\"{start_date.replace('-', '')}_{end_date.replace('-', '')}\" if start_date and end_date else execution_date\n",
    "\n",
    "# Save combined results\n",
    "if not combined_final_df.empty:\n",
    "    # Main combined dataset\n",
    "    combined_csv_filename = f\"Combined_MoM_Country_Source_Volume_Dynamic_{date_range_suffix}.csv\"\n",
    "    try:\n",
    "        combined_final_df.to_csv(combined_csv_filename, index=False)\n",
    "        print(f\"âœ… Combined dataset saved to: {combined_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving combined dataset: {e}\")\n",
    "    \n",
    "    # Country mapping report\n",
    "    country_mapping = pd.DataFrame({\n",
    "        'country_code': sorted(set(mysql_countries + lakehouse_countries + ['US'])),\n",
    "    })\n",
    "    country_mapping['data_source'] = country_mapping['country_code'].apply(\n",
    "        lambda x: 'mysql' if x in mysql_countries else 'athena'\n",
    "    )\n",
    "    country_mapping['schema_type'] = country_mapping.apply(\n",
    "        lambda row: f\"positions_{row['country_code'].lower()}_v1\" if row['data_source'] == 'mysql' \n",
    "                    else 'positions_us_v1' if row['country_code'] == 'US'\n",
    "                    else 'meth_2012', axis=1\n",
    "    )\n",
    "    \n",
    "    mapping_filename = f\"Country_Source_Mapping_{date_range_suffix}.csv\"\n",
    "    try:\n",
    "        country_mapping.to_csv(mapping_filename, index=False)\n",
    "        print(f\"âœ… Country mapping saved to: {mapping_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving country mapping: {e}\")\n",
    "    \n",
    "    # Summary statistics by date range\n",
    "    summary_stats = combined_final_df.groupby(['data_source', 'country_code', 'position_month']).agg({\n",
    "        'total_jobs': 'sum',\n",
    "        'total_positions': 'sum',\n",
    "        'source_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    summary_stats.columns = ['data_source', 'country_code', 'position_month', 'total_jobs_sum', 'total_positions_sum', 'unique_sources']\n",
    "    \n",
    "    summary_filename = f\"Summary_Stats_Dynamic_{date_range_suffix}.csv\"\n",
    "    try:\n",
    "        summary_stats.to_csv(summary_filename, index=False)\n",
    "        print(f\"âœ… Summary statistics saved to: {summary_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving summary statistics: {e}\")\n",
    "    \n",
    "    # Monthly aggregation for trend analysis\n",
    "    monthly_trends = combined_final_df.groupby(['position_month', 'data_source']).agg({\n",
    "        'total_jobs': 'sum',\n",
    "        'total_positions': 'sum',\n",
    "        'country_code': 'nunique'\n",
    "    }).reset_index()\n",
    "    monthly_trends.columns = ['position_month', 'data_source', 'total_jobs', 'total_positions', 'unique_countries']\n",
    "    \n",
    "    trends_filename = f\"Monthly_Trends_Dynamic_{date_range_suffix}.csv\"\n",
    "    try:\n",
    "        monthly_trends.to_csv(trends_filename, index=False)\n",
    "        print(f\"âœ… Monthly trends saved to: {trends_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving monthly trends: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No combined data to save\")\n",
    "\n",
    "# Save timing report if available\n",
    "if not mysql_timing_df.empty:\n",
    "    timing_filename = f\"MySQL_Timing_Report_Dynamic_{date_range_suffix}.csv\"\n",
    "    try:\n",
    "        mysql_timing_df.to_csv(timing_filename, index=False)\n",
    "        print(f\"âœ… MySQL timing report saved to: {timing_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving timing report: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“Š DYNAMIC DATA ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "ðŸ“… Analysis Period: 2025-06-01 to 2025-06-30\n",
      "ðŸ“Š Total Records: 22,392\n",
      "ðŸŒ Countries Covered: 47\n",
      "ðŸ“š Languages: 43\n",
      "ðŸ¢ Unique Sources: 8151\n",
      "ðŸ’¼ Total Jobs: 6,502,419\n",
      "ðŸ“ Total Positions: 4,738,223\n",
      "\n",
      "ðŸŽ¯ DATA SOURCE STRATEGY EXECUTED:\n",
      "   MySQL Individual Schemas: 45 countries\n",
      "   Lakehouse meth_2012: 204 countries\n",
      "   Lakehouse positions_us_v1: US\n",
      "\n",
      "ðŸ“Š MYSQL COUNTRIES:\n",
      "   FR: 1,675,495 jobs (via positions_fr_v1)\n",
      "   GB: 1,219,272 jobs (via positions_gb_v1)\n",
      "   CO: 871,312 jobs (via positions_co_v1)\n",
      "   AU: 544,950 jobs (via positions_au_v1)\n",
      "   CA: 499,760 jobs (via positions_ca_v1)\n",
      "   PE: 371,406 jobs (via positions_pe_v1)\n",
      "   CL: 138,527 jobs (via positions_cl_v1)\n",
      "   ID: 120,941 jobs (via positions_id_v1)\n",
      "   PT: 92,852 jobs (via positions_pt_v1)\n",
      "   UA: 89,651 jobs (via positions_ua_v1)\n",
      "   TW: 72,929 jobs (via positions_tw_v1)\n",
      "   PH: 66,970 jobs (via positions_ph_v1)\n",
      "   HK: 59,588 jobs (via positions_hk_v1)\n",
      "   VN: 54,373 jobs (via positions_vn_v1)\n",
      "   PA: 51,217 jobs (via positions_pa_v1)\n",
      "   SV: 45,424 jobs (via positions_sv_v1)\n",
      "   NI: 31,266 jobs (via positions_ni_v1)\n",
      "   HN: 22,540 jobs (via positions_hn_v1)\n",
      "   KE: 20,210 jobs (via positions_ke_v1)\n",
      "   VE: 20,061 jobs (via positions_ve_v1)\n",
      "   UY: 18,553 jobs (via positions_uy_v1)\n",
      "   EC: 17,449 jobs (via positions_ec_v1)\n",
      "   NG: 16,978 jobs (via positions_ng_v1)\n",
      "   DO: 16,348 jobs (via positions_do_v1)\n",
      "   LT: 15,234 jobs (via positions_lt_v1)\n",
      "   EG: 10,689 jobs (via positions_eg_v1)\n",
      "   LV: 10,655 jobs (via positions_lv_v1)\n",
      "   GT: 10,237 jobs (via positions_gt_v1)\n",
      "   PK: 10,009 jobs (via positions_pk_v1)\n",
      "   BD: 8,188 jobs (via positions_bd_v1)\n",
      "   QA: 6,596 jobs (via positions_qa_v1)\n",
      "   RS: 5,440 jobs (via positions_rs_v1)\n",
      "   MA: 4,824 jobs (via positions_ma_v1)\n",
      "   IL: 4,531 jobs (via positions_il_v1)\n",
      "   BG: 4,112 jobs (via positions_bg_v1)\n",
      "   GR: 3,372 jobs (via positions_gr_v1)\n",
      "   KW: 2,811 jobs (via positions_kw_v1)\n",
      "   SK: 2,513 jobs (via positions_sk_v1)\n",
      "   HR: 2,373 jobs (via positions_hr_v1)\n",
      "   OM: 1,983 jobs (via positions_om_v1)\n",
      "   BH: 1,641 jobs (via positions_bh_v1)\n",
      "   GH: 1,350 jobs (via positions_gh_v1)\n",
      "   MT: 1,010 jobs (via positions_mt_v1)\n",
      "   BA: 436 jobs (via positions_ba_v1)\n",
      "   HRF: 32 jobs (via positions_hrf_v1)\n",
      "\n",
      "ðŸ¢ LAKEHOUSE COUNTRIES:\n",
      "   AR: 216,809 jobs (via meth_2012)\n",
      "   AE: 39,502 jobs (via meth_2012)\n",
      "\n",
      "ðŸ” TOP 10 COUNTRIES BY TOTAL JOBS:\n",
      "   FR: 1,675,495 jobs (MySQL)\n",
      "   GB: 1,219,272 jobs (MySQL)\n",
      "   CO: 871,312 jobs (MySQL)\n",
      "   AU: 544,950 jobs (MySQL)\n",
      "   CA: 499,760 jobs (MySQL)\n",
      "   PE: 371,406 jobs (MySQL)\n",
      "   AR: 216,809 jobs (Lakehouse)\n",
      "   CL: 138,527 jobs (MySQL)\n",
      "   ID: 120,941 jobs (MySQL)\n",
      "   PT: 92,852 jobs (MySQL)\n",
      "\n",
      "ðŸ“ˆ DATA SOURCE BREAKDOWN:\n",
      "   ATHENA: 256,311 jobs, 140,294 positions, 2 countries\n",
      "   MYSQL: 6,246,108 jobs, 4,597,929 positions, 45 countries\n",
      "\n",
      "ðŸ“… MONTHLY DISTRIBUTION:\n",
      "   Jun-25: 6,502,419 jobs\n",
      "\n",
      "================================================================================\n",
      "ðŸ“ Files Generated:\n",
      "   â€¢ Combined_MoM_Country_Source_Volume_Dynamic_20250601_20250630.csv\n",
      "   â€¢ Country_Source_Mapping_20250601_20250630.csv\n",
      "   â€¢ Summary_Stats_Dynamic_20250601_20250630.csv\n",
      "   â€¢ Monthly_Trends_Dynamic_20250601_20250630.csv\n",
      "   â€¢ MySQL_Timing_Report_Dynamic_20250601_20250630.csv\n",
      "\n",
      "ðŸŽ‰ DYNAMIC COUNTRY DETECTION SUCCESSFUL!\n",
      "   No manual country lists required - all countries automatically detected and optimally sourced.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive analysis summary\n",
    "if not combined_final_df.empty and start_date and end_date:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š DYNAMIC DATA ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ“… Analysis Period: {start_date} to {end_date}\")\n",
    "    print(f\"ðŸ“Š Total Records: {len(combined_final_df):,}\")\n",
    "    print(f\"ðŸŒ Countries Covered: {combined_final_df['country_code'].nunique()}\")\n",
    "    print(f\"ðŸ“š Languages: {combined_final_df['language'].nunique()}\")\n",
    "    print(f\"ðŸ¢ Unique Sources: {combined_final_df['source_id'].nunique()}\")\n",
    "    \n",
    "    total_jobs = combined_final_df['total_jobs'].sum()\n",
    "    total_positions = combined_final_df['total_positions'].sum()\n",
    "    print(f\"ðŸ’¼ Total Jobs: {total_jobs:,}\")\n",
    "    print(f\"ðŸ“ Total Positions: {total_positions:,}\")\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ DATA SOURCE STRATEGY EXECUTED:\")\n",
    "    print(f\"   MySQL Individual Schemas: {len(mysql_countries)} countries\")\n",
    "    print(f\"   Lakehouse meth_2012: {len(lakehouse_countries)} countries\")\n",
    "    print(f\"   Lakehouse positions_us_v1: US\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š MYSQL COUNTRIES:\")\n",
    "    mysql_data = combined_final_df[combined_final_df['data_source'] == 'mysql']\n",
    "    if not mysql_data.empty:\n",
    "        mysql_country_stats = mysql_data.groupby('country_code')['total_jobs'].sum().sort_values(ascending=False)\n",
    "        for country, jobs in mysql_country_stats.items():\n",
    "            print(f\"   {country}: {jobs:,} jobs (via positions_{country.lower()}_v1)\")\n",
    "    else:\n",
    "        print(\"   No MySQL data available\")\n",
    "    \n",
    "    print(\"\\nðŸ¢ LAKEHOUSE COUNTRIES:\")\n",
    "    athena_data = combined_final_df[combined_final_df['data_source'] == 'athena']\n",
    "    if not athena_data.empty:\n",
    "        athena_country_stats = athena_data.groupby('country_code')['total_jobs'].sum().sort_values(ascending=False)\n",
    "        for country, jobs in athena_country_stats.items():\n",
    "            schema_type = 'positions_us_v1' if country == 'US' else 'meth_2012'\n",
    "            print(f\"   {country}: {jobs:,} jobs (via {schema_type})\")\n",
    "    else:\n",
    "        print(\"   No lakehouse data available\")\n",
    "    \n",
    "    print(\"\\nðŸ” TOP 10 COUNTRIES BY TOTAL JOBS:\")\n",
    "    top_countries = combined_final_df.groupby('country_code')['total_jobs'].sum().sort_values(ascending=False).head(10)\n",
    "    for country, jobs in top_countries.items():\n",
    "        source = 'MySQL' if country in mysql_countries else 'Lakehouse'\n",
    "        print(f\"   {country}: {jobs:,} jobs ({source})\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ DATA SOURCE BREAKDOWN:\")\n",
    "    source_breakdown = combined_final_df.groupby('data_source').agg({\n",
    "        'total_jobs': 'sum',\n",
    "        'total_positions': 'sum',\n",
    "        'country_code': 'nunique'\n",
    "    })\n",
    "    for source in source_breakdown.index:\n",
    "        jobs = source_breakdown.loc[source, 'total_jobs']\n",
    "        positions = source_breakdown.loc[source, 'total_positions']\n",
    "        countries = source_breakdown.loc[source, 'country_code']\n",
    "        print(f\"   {source.upper()}: {jobs:,} jobs, {positions:,} positions, {countries} countries\")\n",
    "    \n",
    "    print(\"\\nðŸ“… MONTHLY DISTRIBUTION:\")\n",
    "    monthly_dist = combined_final_df.groupby('position_month')['total_jobs'].sum().sort_index()\n",
    "    for month, jobs in monthly_dist.items():\n",
    "        print(f\"   {month}: {jobs:,} jobs\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ðŸ“ Files Generated:\")\n",
    "    print(f\"   â€¢ Combined_MoM_Country_Source_Volume_Dynamic_{date_range_suffix}.csv\")\n",
    "    print(f\"   â€¢ Country_Source_Mapping_{date_range_suffix}.csv\")\n",
    "    print(f\"   â€¢ Summary_Stats_Dynamic_{date_range_suffix}.csv\")\n",
    "    print(f\"   â€¢ Monthly_Trends_Dynamic_{date_range_suffix}.csv\")\n",
    "    if not mysql_timing_df.empty:\n",
    "        print(f\"   â€¢ MySQL_Timing_Report_Dynamic_{date_range_suffix}.csv\")\n",
    "    print(\"\\nðŸŽ‰ DYNAMIC COUNTRY DETECTION SUCCESSFUL!\")\n",
    "    print(\"   No manual country lists required - all countries automatically detected and optimally sourced.\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for analysis summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MySQL connection closed.\n",
      "\n",
      "ðŸŽ‰ Script execution completed!\n",
      "ðŸ“Š Final dataset contains 22,392 rows across 47 countries\n",
      "ðŸ“… Date range analyzed: 2025-06-01 to 2025-06-30\n",
      "ðŸ—‚ï¸ Files saved with date range suffix: 20250601_20250630\n",
      "â±ï¸ Analysis covers 30 days\n",
      "ðŸ¤– Dynamic country detection: 45 from MySQL + 204 from Lakehouse + US\n"
     ]
    }
   ],
   "source": [
    "# Close connections\n",
    "if mysql_conn and mysql_conn.is_connected():\n",
    "    mysql_conn.close()\n",
    "    print(\"âœ… MySQL connection closed.\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Script execution completed!\")\n",
    "if not combined_final_df.empty and start_date and end_date:\n",
    "    print(f\"ðŸ“Š Final dataset contains {len(combined_final_df):,} rows across {combined_final_df['country_code'].nunique()} countries\")\n",
    "    print(f\"ðŸ“… Date range analyzed: {start_date} to {end_date}\")\n",
    "    print(f\"ðŸ—‚ï¸ Files saved with date range suffix: {date_range_suffix}\")\n",
    "    \n",
    "    # Calculate date range span\n",
    "    from datetime import datetime\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    days_span = (end_dt - start_dt).days + 1\n",
    "    print(f\"â±ï¸ Analysis covers {days_span} days\")\n",
    "    print(f\"ðŸ¤– Dynamic country detection: {len(mysql_countries)} from MySQL + {len(lakehouse_countries)} from Lakehouse + US\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
